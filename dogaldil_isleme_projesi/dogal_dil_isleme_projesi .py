# -*- coding: utf-8 -*-
"""Dogal_dil_isleme_projesi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m1_2VuuHoiaQ-9PqmhZqd35si3bqX0i3
"""

import pandas as pd

# 1. Veriyi yükle
df = pd.read_csv('haberler.csv')

# 2. Kategorileri ana gruplara ata
spor_list = ['spor', 'futbol', 'basketbol', 'tenis', 'voleybol', 'motor-sporlari', 'dunya-ligleri']
ekonomi_list = ['ekonomi', 'otomotiv', 'emlak', 'otomobil']

def kategori_duzenle(cat):
    if cat in spor_list:
        return 'spor'
    elif cat in ekonomi_list:
        return 'ekonomi'
    else:
        return 'gundem'

df['kategori'] = df['kategori'].apply(kategori_duzenle)

# 3. Her kategoriden 3980 örnek seçerek veriyi dengele
# random_state=42 sayesinde her çalıştırdığında aynı satırları seçer
dengeli_df = df.groupby('kategori').apply(lambda x: x.sample(n=3980, random_state=42)).reset_index(drop=True)

# 4. Kaydet
dengeli_df[['haber', 'kategori']].to_csv('haberler_temizlenmiş.csv', index=False, encoding='utf-8-sig')

print("Veri seti dengelendi!")
print(dengeli_df['kategori'].value_counts())

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import re
from collections import Counter
import nltk
from sklearn.feature_extraction.text import CountVectorizer

from nltk.corpus import stopwords

nltk.download("stopwords")
stop_words_eng= set(stopwords.words("english"))

# Eğer CSV ise:
df = pd.read_csv('haberler_temizlenmiş.csv', on_bad_lines='skip')
df=df[:]
documents = df['haber']
labels = df['kategori']
print(df.head())

import re

def cleaned_text(text):
    # Handle non-string inputs (like NaN) by converting them to an empty string
    if not isinstance(text, str):
        return ""
    text=re.sub(r"[^a-zçğıiöşü\s]", "", text)
    text=text.lower()#küçük harfe çevir
    #rakamları kaldırır
    #özel karakterler kaldırır
    text=re.sub(r"[^A-Za-z0-9\s]", "", text)#textteki harf ve rakam haricindeki herşeyi kaldırır
    #kısa kelimelerin temizlenmesi
    text= " ".join([word for word in text.split() if len(word) > 2])

    return text

cleaned_doc= [cleaned_text(row) for row in documents]
#vectorizeri tanımlar
vectorizer=CountVectorizer()

#metni->sayısal hale getirir
X=vectorizer.fit_transform(cleaned_doc[:])

#kelime kumesi getirir
feature_names=vectorizer.get_feature_names_out()

#vektör temsili
vector_temsili=X.toarray()
print(f"Kelime kümesi: {vector_temsili}")

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter

# --- 1. BAG-OF-WORDS ANALİZİ (İSTATİSTİK) ---
vectorizer_bow = CountVectorizer()
X_bow = vectorizer_bow.fit_transform(cleaned_doc_processed)
feature_names = vectorizer_bow.get_feature_names_out()

# En sık geçen 5 kelimeyi bulalım
word_counts = X_bow.sum(axis=0).A1
word_freq = dict(zip(feature_names, word_counts))
most_common_5_words = Counter(word_freq).most_common(5)
print(f"Bag-of-Words Analizi - En Sık Geçen 5 Kelime: {most_common_5_words}")

# --- 2. BAG-OF-WORDS SINIFLANDIRMA (MODEL) ---
X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(
    X_bow, labels_processed, test_size=0.2, random_state=42
)

model_bow = MultinomialNB()
model_bow.fit(X_train_bow, y_train_bow)

y_pred_bow = model_bow.predict(X_test_bow)
acc_bow = accuracy_score(y_test_bow, y_pred_bow)
print(f"Bag-of-Words Model Başarı Puanı: %{acc_bow*100:.2f}")

# NLTK Türkçe stop words
nltk.download("stopwords")
from nltk.corpus import stopwords
turkish_stopwords = stopwords.words('turkish')

# Fonksiyonundaki 'turkish_stopwords' artık bu listeyi kullanacak.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['haber'], df['kategori'], test_size=0.2, random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# 1. Etiketlerdeki NaN değerlerini ele alın ve X ile Y'nin hizalanmasını sağlayın
# İlk olarak, NaN'leri kolayca atmak için geçici bir DataFrame oluşturun

temp_df = pd.DataFrame({'text': cleaned_doc, 'label': labels})
temp_df.dropna(subset=['label'], inplace=True)

cleaned_doc_processed = temp_df['text'].tolist()
labels_processed = temp_df['label'].tolist()

# Veriyi Ayırma
X_train, X_test, y_train, y_test = train_test_split(cleaned_doc_processed, labels_processed, test_size=0.2, random_state=42)

# 2. TF-IDF Temsili
tfidf_vec = TfidfVectorizer()
X_train_tfidf = tfidf_vec.fit_transform(X_train)
X_test_tfidf = tfidf_vec.transform(X_test)

# 3. N-Gram Temsili (Bigram: ikili kelime grupları)
ngram_vec = CountVectorizer(ngram_range=(1, 2))
X_train_ngram = ngram_vec.fit_transform(X_train)
X_test_ngram = ngram_vec.transform(X_test)

# Örnek Sınıflandırma (TF-IDF için)
clf = MultinomialNB()
clf.fit(X_train_tfidf, y_train)
y_pred = clf.predict(X_test_tfidf)

print("TF-IDF Modeli Başarı Raporu:")
print(classification_report(y_test, y_pred))

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# N-Gram (Bigram: hem tekli hem ikili kelimeleri alıyoruz)
ngram_vec = CountVectorizer(ngram_range=(1, 2))
X_train_ngram = ngram_vec.fit_transform(X_train)
X_test_ngram = ngram_vec.transform(X_test)

# Model Eğitimi
model_ngram = MultinomialNB()
model_ngram.fit(X_train_ngram, y_train)

# Tahmin
y_pred_ngram = model_ngram.predict(X_test_ngram)
print("--- N-Gram (1,2) Başarı Raporu ---")
print(classification_report(y_test, y_pred_ngram, zero_division=0))

!pip install gensim

from gensim.models import Word2Vec
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Kelimelere ayırma
train_sentences = [text.split() for text in X_train]

# Modeli eğitme
w2v_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, min_count=1)

# Cümleleri vektör ortalamasına çeviren fonksiyon
def vectorize(sentence):
    words = sentence.split()
    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]
    if len(words_vecs) == 0:
        return np.zeros(100)
    return np.mean(words_vecs, axis=0)

X_train_w2v = np.array([vectorize(s) for s in X_train])
X_test_w2v = np.array([vectorize(s) for s in X_test])

# Word2Vec ile Random Forest daha iyi sonuç verebilir
model_w2v = RandomForestClassifier(n_estimators=100)
model_w2v.fit(X_train_w2v, y_train)
print("Word2Vec Modeli Hazır.")

# Gerekli kütüphane: pip install sentence-transformers
from sentence_transformers import SentenceTransformer

# Türkçe için en iyi çalışan modellerden biri
bert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Metinleri vektörlere dönüştürme (Bu işlem biraz zaman alabilir)
X_train_bert = bert_model.encode(X_train,show_progress_bar=True)
X_test_bert = bert_model.encode(X_test,show_progress_bar=True)

# Sınıflandırma
model_bert = RandomForestClassifier()
model_bert.fit(X_train_bert, y_train)
y_pred_bert = model_bert.predict(X_test_bert)
print("--- BERT Başarı Raporu ---")
print(classification_report(y_test, y_pred_bert, zero_division=0))

import pandas as pd
import matplotlib.pyplot as plt

# 1. Boş bir liste oluşturuyoruz (Tüm yöntemler burada toplanacak)
performans_sonuclari = []

# --- 1. BAG-OF-WORDS ---
performans_sonuclari.append({"Yöntem": "Bag-of-Words", "Doğruluk": acc_bow})

# --- 2. TF-IDF ---
acc_tfidf = accuracy_score(y_test, y_pred)
performans_sonuclari.append({"Yöntem": "TF-IDF", "Doğruluk": acc_tfidf})

# --- 3. N-GRAM (1,2) ---
acc_ngram = accuracy_score(y_test, y_pred_ngram)
performans_sonuclari.append({"Yöntem": "N-Gram (1,2)", "Doğruluk": acc_ngram})

# --- 4. WORD2VEC ---
y_pred_w2v = model_w2v.predict(X_test_w2v)
acc_w2v = accuracy_score(y_test, y_pred_w2v)
performans_sonuclari.append({"Yöntem": "Word2Vec", "Doğruluk": acc_w2v})

# --- 5. BERT (TRANSFORMERS) ---
acc_bert = accuracy_score(y_test, y_pred_bert)
performans_sonuclari.append({"Yöntem": "BERT (Transformers)", "Doğruluk": acc_bert})

# 2. Veriyi DataFrame'e çevir ve başarıya göre sırala (Daha şık görünür)
df_sonuc = pd.DataFrame(performans_sonuclari)
df_sonuc = df_sonuc.sort_values(by="Doğruluk", ascending=False)

# 3. Final Grafiği Çizdir
plt.figure(figsize=(12, 7))
renkler = ['#2ecc71', '#3498db', '#9b59b6', '#f1c40f', '#e74c3c'] # Profesyonel renkler
bars = plt.bar(df_sonuc["Yöntem"], df_sonuc["Doğruluk"], color=renkler)

# Çubukların üzerine başarı yüzdelerini yazalım
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f"%{yval*100:.1f}",
             ha='center', fontweight='bold', fontsize=12)

plt.title("Metin Temsil Yöntemlerinin Başarı Karşılaştırması", fontsize=15)
plt.ylabel("Accuracy (Doğruluk)", fontsize=12)
plt.ylim(0, 1.1) # Üstten biraz boşluk kalsın
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

print("\n--- FİNAL BAŞARI TABLOSU ---")
print(df_sonuc)

import numpy as np

def haber_analiz_et(yeni_metin, model, vectorizer):
    # 1. Metni temizle
    temiz_metin = cleaned_text(yeni_metin)

    # 2. Vektöre çevir
    vektor = vectorizer.transform([temiz_metin])

    # 3. Sınıfı tahmin et
    tahmin_edilen_sinif = model.predict(vektor)[0]

    # 4. Güven skorunu hesapla (Olasılıklar içinden en yükseğini al)
    olasiliklar = model.predict_proba(vektor)[0]
    en_yuksek_olasilik = np.max(olasiliklar)

    return tahmin_edilen_sinif, en_yuksek_olasilik

# --- SUNUM ANINDA KULLANIM ---
test_haberi = input("Lütfen analiz edilecek haber metnini giriniz: ")

# TF-IDF modelini kullanarak tahmin yapalım
sinif, skor = haber_analiz_et(test_haberi, clf, tfidf_vec)

print("-" * 30)
print(f"Tahmin Edilen Kategori: {sinif.upper()}")
print(f"Güven Skoru: %{skor*100:.2f}")
print("-" * 30)